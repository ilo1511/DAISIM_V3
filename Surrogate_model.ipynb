{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCw1O1rXU68y"
      },
      "source": [
        "1) Import libraries and turn CSV file into dataframe and delete rows with empty cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77UaTuimU68z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/sample_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows with missing values\n",
        "data_cleaned = data.dropna()\n",
        "\n",
        "# Define features and targets\n",
        "X = data_cleaned[['belief', 'hlow', 'hhigh', 'alpha1', 'alpha2', 'low_herding_share']]\n",
        "y = data_cleaned[['correlation', 'rmse']].values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.5) Perform data transformation if needed. In this case, we dublicate the samples with a correlation that is different then the most frequent one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRACulBCf3rE",
        "outputId": "d648618a-1a83-4915-862e-b9259acef092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset size: 7240\n",
            "Weighted dataset size: 27224\n"
          ]
        }
      ],
      "source": [
        "# Extract the 'correlation' column\n",
        "correlation = y[:, 0]\n",
        "\n",
        "# The two most frequent values provided\n",
        "frequent_value1 = 0.430170230794728\n",
        "frequent_value2 = -0.430170231\n",
        "\n",
        "# Assign weights to samples\n",
        "weights = np.ones_like(correlation)\n",
        "weights[(correlation > frequent_value1) & (correlation < frequent_value2)] = 100\n",
        "weights[(correlation < frequent_value1) & (correlation > frequent_value2)] = 5\n",
        "\n",
        "# Duplicate the rows based on the weights\n",
        "X_weighted = np.repeat(X, weights.astype(int), axis=0)\n",
        "y_weighted = np.repeat(y, weights.astype(int), axis=0)\n",
        "\n",
        "print(f\"Original dataset size: {X.shape[0]}\")\n",
        "print(f\"Weighted dataset size: {X_weighted.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj8it19RU680"
      },
      "source": [
        "2) Split data into training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYT-mMSdU680",
        "outputId": "c1270692-aaa8-4be3-ded9-f705c8dc1d19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 21779\n",
            "Testing set size: 5445\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test= train_test_split(\n",
        "    X_weighted,y_weighted, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure y_train and y_test are NumPy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC5ZAkIZU680"
      },
      "source": [
        "3) Build a Neural Network and Gradient Boosting Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3eZ2dsuU680"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "mlp = MLPRegressor(random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSL-NkEbVCqD",
        "outputId": "e3657713-f598-4e3c-c2af-0b676856aa87"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 234 is smaller than n_iter=300. Running 234 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters for MLPRegressor:  {'solver': 'lbfgs', 'max_iter': 1500, 'learning_rate_init': 0.1, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (6, 7, 7, 5), 'activation': 'relu'}\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Generate hidden layer combinations\n",
        "def generate_hidden_layer_combinations(layers, neurons):\n",
        "    return [combo for layer in layers for combo in itertools.product(neurons, repeat=layer)]\n",
        "\n",
        "# Define the number of layers and neurons per layer\n",
        "num_layers = [2,3,4]\n",
        "neurons_per_layer = [5,6,7]\n",
        "\n",
        "# Generate hidden layer combinations\n",
        "hidden_layers_mlp = generate_hidden_layer_combinations(num_layers, neurons_per_layer)\n",
        "\n",
        "param_dist_mlp = {\n",
        "    'hidden_layer_sizes': hidden_layers_mlp,\n",
        "    'activation': ['relu'],\n",
        "    'solver': ['lbfgs'],\n",
        "    'learning_rate': ['adaptive'],\n",
        "    'learning_rate_init': [0.1],\n",
        "    'max_iter': [1000, 1500],\n",
        "}\n",
        "\n",
        "# Initialize the MLPRegressor\n",
        "mlp = MLPRegressor()\n",
        "\n",
        "# Perform RandomizedSearchCV for MLPRegressor\n",
        "random_search_mlp = RandomizedSearchCV(estimator=mlp, param_distributions=param_dist_mlp, n_iter=300,\n",
        "                                       cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1, verbose=0)\n",
        "random_search_mlp.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best hyperparameters for MLPRegressor: \", random_search_mlp.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kE8pkjiyU680",
        "outputId": "ef5097ad-096e-451e-d339-907059106ce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 72 is smaller than n_iter=300. Running 72 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters for GradientBoostingRegressor:  {'estimator__n_estimators': 20, 'estimator__min_samples_split': 2, 'estimator__min_samples_leaf': 2, 'estimator__max_depth': 10, 'estimator__learning_rate': 0.1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Define the parameter grid for Gradient Boosting Regressor\n",
        "param_dist_gbr = {\n",
        "    'estimator__n_estimators': [10, 15, 20],\n",
        "    'estimator__learning_rate': [0.01, 0.1],\n",
        "    'estimator__max_depth': [6, 8, 10],\n",
        "    'estimator__min_samples_split': [2, 3],\n",
        "    'estimator__min_samples_leaf': [1, 2],\n",
        "}\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "# Initialize the MultiOutputRegressor\n",
        "multi_gbr = MultiOutputRegressor(gbr)\n",
        "\n",
        "# Perform RandomizedSearchCV for MultiOutputRegressor with Gradient Boosting Regressor\n",
        "random_search_gbr = RandomizedSearchCV(estimator=multi_gbr, param_distributions=param_dist_gbr, n_iter=300,\n",
        "                                       cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the model with sample weights for the first output\n",
        "random_search_gbr.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters for GradientBoostingRegressor: \", random_search_gbr.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCUZ93QPU681"
      },
      "source": [
        "4) Evaluate both model on the test set to chose the \"winner\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2reFrBQ1U681",
        "outputId": "fcbfec12-a53f-4442-b1b1-a6b812fdef47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GradientBoostingRegressor - MSE Output:  0.028086878026668774\n",
            "MLPRegressor - MSE Output:  0.045639675394935894\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "best_gbr = random_search_gbr.best_estimator_\n",
        "best_mlp = random_search_mlp.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_test_pred_gbr = pd.DataFrame(best_gbr.predict(X_test))\n",
        "y_test_pred_mlp = pd.DataFrame(best_mlp.predict(X_test))\n",
        "\n",
        "# Calculate MSE for each output\n",
        "mse_mlp = mean_squared_error(y_test, y_test_pred_mlp)\n",
        "\n",
        "mse_gbr = mean_squared_error(y_test, y_test_pred_gbr)\n",
        "\n",
        "# Print individual MSEs\n",
        "print(\"GradientBoostingRegressor - MSE Output: \", mse_gbr)\n",
        "\n",
        "print(\"MLPRegressor - MSE Output: \", mse_mlp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRsc5Ll1U681"
      },
      "source": [
        "5) Use the newly built surrogate model to generate 10 000 000 parameter combinations, pass them through the surrogate model and find the combination leading to the lowest output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8LtI6WIU681",
        "outputId": "e814e827-7c01-4505-c113-084999511b23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 combinations with the highest correlation:\n",
            "            belief      hlow     hhigh    alpha1    alpha2  low_herding_share  \\\n",
            "5818615   8.362252  0.489853  0.578984  0.075125  0.719272           0.228310   \n",
            "8715712   8.272141  0.489689  0.720229  1.211329  0.025202           0.216874   \n",
            "7410921   8.226318  0.492748  0.769365  1.344162  0.070500           0.105055   \n",
            "6579176   9.047509  0.496105  0.607718  1.377633  0.089836           0.197890   \n",
            "494158   11.779117  0.312089  0.940379  0.047494  0.060807           0.137979   \n",
            "\n",
            "         correlation      rmse     error  \n",
            "5818615     0.471023  0.021483  0.224481  \n",
            "8715712     0.466320  0.022940  0.227236  \n",
            "7410921     0.447787  0.021179  0.233592  \n",
            "6579176     0.444062  0.022897  0.236114  \n",
            "494158      0.435371  0.020474  0.238136  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.3173862"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate new inputs within the specified parameter bounds\n",
        "def generate_random_params(param_bounds, n_iterations):\n",
        "    params = {}\n",
        "    for param, (low, high) in param_bounds.items():\n",
        "        params[param] = np.random.uniform(low, high, n_iterations)\n",
        "    return pd.DataFrame(params)\n",
        "\n",
        "# Define the parameter bounds\n",
        "param_bounds = {\n",
        "    'belief': (8, 13),\n",
        "    'hlow': (0, 0.5),\n",
        "    'hhigh': (0.5, 1),\n",
        "    'alpha1': (0, 5),\n",
        "    'alpha2': (0, 5),\n",
        "    'low_herding_share': (0, 1)\n",
        "}\n",
        "\n",
        "# Generate 10 million random parameter combinations\n",
        "n_iterations = 10000000\n",
        "np.random.seed(42)\n",
        "new_combinations = generate_random_params(param_bounds, n_iterations)\n",
        "\n",
        "# Use the best model to predict the outputs for the new inputs\n",
        "best_model = random_search_gbr.best_estimator_\n",
        "predictions = best_model.predict(new_combinations)\n",
        "\n",
        "# Add the predictions to the new inputs DataFrame\n",
        "new_combinations['correlation'] = predictions[:, 0]\n",
        "new_combinations['rmse'] = predictions[:, 1]\n",
        "\n",
        "new_combinations['error'] = 0.4*(1-new_combinations['correlation']) + 0.6*new_combinations['rmse']\n",
        "\n",
        "\n",
        "# Display the 5 combinations that output the highest 'correlation'\n",
        "top_5_combinations = new_combinations.nsmallest(5, 'error')\n",
        "print(\"Top 5 combinations with the highest correlation:\")\n",
        "print(top_5_combinations)\n",
        "\n",
        "0.3173862\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
